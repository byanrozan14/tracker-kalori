# -*- coding: utf-8 -*-
"""tracker.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tPCJoIjU4FabE5Ou1_BPI3zhmukrIqan
"""

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d openfoodfacts/world-food-facts

import zipfile
import pandas as pd

# Ekstrak
with zipfile.ZipFile("world-food-facts.zip", 'r') as zip_ref:
    zip_ref.extractall("food_data")

# Tampilkan file
import os
os.listdir("food_data")

# Load file utama, misal: en.openfoodfacts.org.products.tsv
df = pd.read_csv("food_data/en.openfoodfacts.org.products.tsv", sep='\t')
df.head()

print("Shape of dataset:", df.shape)

# Kolom yang ingin digunakan
kolom = [
    'energy_100g', #kalori
    'fat_100g',
    'carbohydrates_100g',
    'sugars_100g',
    'proteins_100g',
    'product_name',
    'ingredients_text' ]

kolom  = [col for col in kolom  if col in df.columns]
df = df[kolom ].copy()

# Ubah nama kolom
df.rename(columns={
    "energy_100g": "calories",
    "fat_100g": "fat",
    "carbohydrates_100g": "carb",
    "proteins_100g": "protein",
    "sugars_100g": "sugars",
    "product_name": "name"
}, inplace=True)

df.head()
df.info()

print("\nCek missing values di tiap kolom:")
print(df.isnull().sum())

from sklearn.impute import SimpleImputer

# Kolom numerik yang ingin diisi nilai NaN dengan median
kolom_numerik = [
    'calories',
    'fat',
    'carb',
    'sugars',
    'protein'
]

# Hitung median per kolom
medians = df[kolom_numerik].median()

# Isi nilai NaN dengan median masing-masing kolom
df[kolom_numerik] = df[kolom_numerik].fillna(medians)

# Cek ulang missing value
print("\nCek missing values di tiap kolom:")
print(df.isnull().sum())

# Cek jumlah duplikat
duplikat_count = df.duplicated().sum()
print(f"Jumlah baris duplikat: {duplikat_count}")

# Menghapus baris duplikat
df = df.drop_duplicates().reset_index(drop=True)

# Cek ulang jumlah duplikat
duplikat_count = df.duplicated().sum()
print(f"Jumlah baris duplikat: {duplikat_count}")

# # Batasi kalori pada rentang yang wajar (misal: 0 - 1000 kkal per 100g)
# df = df[(df['calories'] >= 0) & (df['calories'] <= 1000)]

# 2. Normalisasi nama produk  normalisasi (lowercase, strip).
df["name"] = df["name"].astype(str).str.lower().str.strip()

for kolom in ['calories', 'fat', 'carb', 'sugars', 'protein']:
    Q1 = df[kolom].quantile(0.25)
    Q3 = df[kolom].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    df[kolom] = df[kolom].clip(lower=lower_bound, upper=upper_bound)

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df[['calories', 'fat', 'carb', 'sugars', 'protein']] = scaler.fit_transform(
    df[['calories', 'fat', 'carb', 'sugars', 'protein']]
)

"""##EDA"""

# Statistik deskriptif nutrisi
nutrisi_cols = ["calories", "fat", "carb", "protein","sugars"]
df[nutrisi_cols].describe()

import seaborn as sns
import matplotlib.pyplot as plt

# Jika kamu sudah punya DataFrame, misalnya df
plt.figure(figsize=(8, 6))
sns.boxplot(data=df[['calories', 'fat', 'carb', 'sugars', 'protein']])
plt.title('Boxplot untuk Deteksi Outlier pada Data Gizi Makanan')
plt.xlabel('Fitur')
plt.ylabel('Nilai')
plt.grid(True)
plt.xticks(rotation=45)
plt.show()

# scatter plot
plt.figure(figsize=(6, 4))
sns.scatterplot(x=df['fat'], y=df['calories'])
plt.title('Hubungan antara Fat dan kalori per 100g')
plt.xlabel('Fat per 100g')
plt.ylabel('Energy(kkal) per 100g')
plt.show()

# # scatter plot
# plt.figure(figsize=(6, 4))
# sns.scatterplot(x=df['protein'], y=df['calories'])
# plt.title('Hubungan antara protein dan kalori per 100g')
# plt.xlabel('protein per 100g')
# plt.ylabel('Energy(kkal) per 100g')
# plt.show()

# # scatter plot
# plt.figure(figsize=(6, 4))
# sns.scatterplot(x=df['carb'], y=df['calories'])
# plt.title('Hubungan antara carb dan kalori per 100g')
# plt.xlabel('carb per 100g')
# plt.ylabel('Energy(kkal) per 100g')
# plt.show()

# Heatmap
korelasi = df[kolom_numerik].corr(method="pearson")
print("Matriks Korelasi (Pearson):")
display(korelasi.round(2))

plt.figure(figsize=(8, 4))
sns.heatmap(korelasi, annot=True, fmt=".2f", cmap="coolwarm", linewidths=0.5)
plt.title("Heatmap Korelasi Fitur Numerik")
plt.show()

# Hapus baris jika ada nilai nutrisi yang masih NaN (untuk kebutuhan modeling)
df_model = df.dropna(subset=["fat", "carb", "protein", "calories"])

# Pembagian Model
X = df_model[["fat", "carb", "protein", "sugars"]]
y = df_model["calories"]

print(f"Jumlah sampel untuk model: {X.shape[0]}")

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

from sklearn.model_selection import train_test_split

# 80% train, 20% test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print("Data latih:", X_train.shape)
print("Data uji:", X_test.shape)

!pip install xgboost

from xgboost import XGBRegressor
# Dictionary model dan parameter
models = {
    "XGBoost": {
        "model": XGBRegressor(objective='reg:squarederror', random_state=42),
        "params": {
            "n_estimators": [100, 200],
            "max_depth": [3, 5],
            "learning_rate": [0.05, 0.1]
        }
      }
    }

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score

for name, config in models.items():
    print(f"\n--- {name} ---")

    grid = GridSearchCV(config["model"], config["params"], cv=3, scoring="r2", n_jobs=-1)
    grid.fit(X_train, y_train)

    best_model = grid.best_estimator_
    y_pred = best_model.predict(X_test)

    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print("Best Parameters:", grid.best_params_)
    print("MSE:", mse)
    print("R2:", r2)

# Import the joblib library
import joblib

# Save the best model and the scaler
if name == "XGBoost":
    joblib.dump(best_model, 'tracker_kalori.joblib')
    joblib.dump(scaler, 'scaler.joblib')
    print("\nModel and scaler saved successfully!")

import pickle

filename = 'tracker_kalori.sav'
pickle.dump(best_model, open(filename, 'wb'))